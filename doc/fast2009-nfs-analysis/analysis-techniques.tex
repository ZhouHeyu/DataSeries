\section{Analysis techniques}
\label{sec:analysis-techniques}

Analyzing the very large amount of data that we collected meant that
we had to adopt and develop new techniques for analyzing this data.
The most important property that we aimed for was bounded memory,
which meant that we needed to have streaming analysis.  The second
property that we wanted was efficiency, because without efficiency, we
would not be able to analyze complete datasets.  One of our lessons is
that these techniques allow us to handle the much larger datasets that
we have collected.

\subsection{Approximate quantiles}

Simple statistics over storage traces often provide misleading
results.  For example, calculating the mean latency of an I/O will
result in a value that occurs rarely as it will average the cache hit
time and the disk I/O time.  Histograms provide a better way to
understand the distribution of latencies, but require the user to
preemptively determine the buckets for the histogram.  If there are
multiple types of cache hits, they could easily be put into the same
histogram bucket resulting in loss of important information.

The gold standard for calculating these types of statistics would be a
quantile.  The $q-$quantile of a set of $n$ data elements is the
element at position $\lceil q*n\rceil$ in the sorted list of elements
indexed from 1 to n.  Unfortunately calculating exact quantiles
requires buffering all of the operations in memory.  Since we can have
billions of data elements, it is not likely that we would be able to
calculate exact quantiles.

There is an algorithm from the database field for calculating
approximate quantiles in bounded
memory~\cite{Manku98approximatemedians}.  The basic idea is that the
user specifies two numbers $\epsilon$, the maximum error, and $N$, the
maximum number of elements.  Then when the program calculates quantile
$q$, it actually gets a quantile in the range
$[q-\epsilon,q+\epsilon]$.  Provided that the total number of elements
was less than $N$, the bound is guaranteed.  The approximate quantile
algorithm works essentially by keeping a collection of $c$ buffers
each containing $k$ elements.  Until we fill up all our buffers, we
just add values into a buffer.  Once all the buffers are full we need
to collapse two buffers together into a single buffer.  In the
simplest case, we pick two buffers, sort the combined elements, select
every other element to create a new buffer, and assign a weight $w$ to
the new buffer of 2 since each element in the buffer is logically
representing two values.  As the algorithm progresses, it may combine
buffers of differing weights, so it will pick values from a logically
sorted list where each element is repeated $w$ times.  The complexity
in the algorithm is in selecting appropriate values for $c$ and $k$
based on $\epsilon$ and $N$; selecting the right buffers for collapse;
and proving that the resulting buffers at the end provide enough
values to select the approximate quantiles.

The approximate quantile algorithm dramatically reduces the required
memory.  For example, with $\epsilon$ = 0.01, and $N = 10^9$, a
standard quantile would need about 8GB of memory to store the billion
doubles, while the approximate quantile only uses about 60KB.  We
generally chose to output 100 evenly spaced quantiles since we are
going to make graphs from them.  We set $\epsilon = 1/(2*100)$ so we
were guaranteed to have non-overlapping ranges for our 100 output
values.  In testing we have found the approximate quantiles to
generate results about $10$x better than the bound.

\subsection{Cube}

Calculating aggregate or roll-up statistics is an important part of
analyzing a workload.  For example, consider the information in the
common NFS table: $\langle$time, operation, client-id, and
server-id$\rangle$.  We may want to calculate the total number of
operations performed by client 5, in which case we want to count the
number of rows that match $\langle$*, *, 5, *$\rangle$; or we might
want to calculate the number of read operations, in which case we want
to count the number of rows that match $\langle$*, read, *,
*$\rangle$.

The cube\cite{gray97cube} is a generalization of the group-by
operations described above.  Given a collection of rows, it calculates
the set of unique values for each column $U(c)$, adds the special
value `ANY' to the set, and then generates one row for each member of
the cross-product $U(1) \times U(2) \times ... U(n)$.

We implemented a very efficient templated version of the cube operator
for use in data analysis.  We added three features to deal with memory
usage.  First, we distinguish between the cube that only includes rows
with actual values in it, and the cube with all rows in the
cross-product (the zero-cube).  The zero-cube can have a very large
number of entries in it, and if we are using an approximate quantile
for those values we can easily waste a large amount of memory.
Second, we added support to dynamically determine which members of the
cross-product will be cubed.  For example, we have a large number of
client id's, and so we can avoid cubing over entries with the client
specified and also the operation to reduce the number of statistics
calculated.  Third, we added the ability to prune values out of the
cube. For example, we can output cube values for earlier time values
and remove them from the data structure once we reach later time
values since we know the data is sorted by time.

The cube allows us to easily calculate a wide variety of summary
statistics.  We had previously manually implemented some of the
summary statistics by doing explicit roll-ups for some of the
aggregates described in the example.  We discovered that the general
implementation was actually more efficient than our manual one because
it used a single hash table for all of the data rather than nested
data structures, and because we tuned the hash function over the tuple
of values to be efficiently calculated.

\subsection{HashTable}

Our hash-table implementation~\cite{DSOpenSource} is a straightforward
chained-hashing implementation.  In our experiments it is strictly
better in both performance and memory than the C++ hash table.  It
uses somewhat more memory than the Google sparse
hash~\cite{google-sparse-hash}, but performs almost as well as the
dense hash.  Because it uses chaining, it does not need any special
sentinel values as used in the google hash.  
It has three features not normally found in hash-table
implementations.  First, it has a function to report on the memory
usage of the hash table; this is important because tracking memory
usage allows us to determine what we need to optimize.  Second it
includes a function to partially reset an iterator.  Normally after an
erase() operation, all iterators to a data-structure may be
invalidated.  We include a partialReset() function that will restart
an iterator at the beginning of the chain.  This allows us to walk the
hash table and remove values using a single iterator.  Finally, we
provide access to the underlying hash-data.  Sometimes at the end of a
program, we want to output all the values in a hash-table in order.
Normally, you would have to copy the data to a vector and sort the
vector.  However our hash-table allows the program to get access to
the underlying data vector and sort the vector.  While this operation
destroys the hash table, that is generally not a problem since the
program is about to stop.  This reduces the memory used by a factor of
two since the data values do not need to be copied into a separate
vector.

\subsection{Rotating hash-map}

One of the problems during analysis is that you can need to age out
old (key,value) pairs, but you don't know how long you will need to
keep around particular values for any given key.  For example,
sequentiality is a per-file statistic.  So long as accesses are active
to the file, we want to continue to update the run information.  Once
the file becomes inactive for long enough, we want to calculate
summary statistics and remove the general statistics from memory.  One
option would be to keep the values in an LRU data-structure, however if
our analysis only needs to keep a file id and last offset, then we
could easily double the size of our data-structure by keeping the
forward and backward pointers needed for LRU.  Alternately, we could
use a clock-style algorithm, but this would require a full scan over
the entire data-structure on a regular basis.

We chose to solve this problem by keeping two hash-maps, the {\it
recent} and {\it old} hash-maps.  Any time a value is accessed, it is
moved to the recent hash-map if it is not already there.  From time to
time, the program will call the rotate(fn) operation which will apply
fn to all of the (key,value) pairs in the old hash map, delete that map,
assign the recent map to the old map and create a new recent map.

Therefore, if the analysis wants to guarantee any gap of up to 60
seconds will be considered part of the same run, it just needs to call
rotate() every 60 seconds.  Any value accessed in the last 60 seconds
will remain present in the hash-map.  We could reduce the memory
overhead somewhat by keeping more than two hash-maps at the cost of
additional lookups, but we have so far found that the rotating
hash-map provides a good tradeoff between minimizing memory usage and
maximizing performance.  We believe that the LRU approach would be
more effective if the size of the data stored in the hash map were
larger.

\subsection{mercury-plot}

Once we have analyzed the data from our binary format using the cube
and quantile techniques, we need to graph and subset the data.  For
this purpose we use a combination of SQL, perl, and gnuplot.  The use
of SQL makes it very easy to take subsets of the data, for example if
we have data on 60 second intervals, it is easy to calculate
min/mean/max for 3600 second intervals, or with the cube to select out
the subset of the data that we want to use.  We use perl to handle
operations that the database can't handle, for example, in the cube,
we represent the `ANY' value as null, but SQL requires different
syntax to select for null vs. a specific value.  We hide this
difference in the perl functions.  In practice, this allows us to
write very simple commands such as {\tt plot quantile as x, value as y
from nfs\_hostinfo\_cube where operation = 'read' and direction =
'send'} to generate a portion of the graph.  Some of our analysis
explicitly generate SQL as output to make this later analysis easier.
This is one of our lessons, all analysis should include an SQL output
mode that can be used to generate derived graphs and further analyze
the data.
