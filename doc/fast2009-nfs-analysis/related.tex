\section{Related work}
\label{sec:related}

The two closest pieces of related work are Ellard's NFS
study~\cite{EllardFast03,EllardLisa03}, and Leung's 2007 CIFS
study~\cite{LeungUsenix08}.  These papers also summarize the earlier
decade of filesystem tracing, so we refer interested readers to those
papers.  Ellard et al.\ captured NFS traces from a number of Digital
UNIX and NetApp servers on the Harvard campus, analyzed the traces and
presented new results looking at the sequentiality of the workload,
and comparing his results to earlier traces.  Ellard made his tools
available, so we initially considered building on top of them, but
quickly discovered that our workload was so much more intense that his
tools would be insufficient, and so ended up building our own.  We
later translated those tools and traces into DataSeries, and found our
version was about 100$\times$ faster on a four core machine and used
25$\times$ less CPU time for analysis.  Our 2003 traces were about
25$\times$ more intense than Ellard's 2001 traces, and about 6$\times$
more intense than Ellard's 2003 traces.

Leung et al.\ traced a pair of NetApp servers on their campus.
Since the clients were entirely running the Windows operating system, his traces were of
CIFS data, and so he used the Wireshark tools~\cite{wireshark} to
convert the traces.  Leung's traces were of comparable intensity to
Ellard's traces, and they noted that they had some small packet drops
during high load as they just used tcpdump for capture.  Leung identified and extensively analyzed 
complicated sequentiality patterns.
Our 2007 traces were about 95$\times$ more intense than Leung's
traces, as they saw a peak of 19.1 million operations/day and we saw an
average of about 1.8 billion.  This comparison is slightly misleading
as NFS tends to have more operations than CIFS because NFS is a
stateless protocol.

Tcpdump~\cite{tcpdump} is the tool that almost all researchers describe using to capture packet
traces.  We tried using tcpdump, but experienced massive
packet loss using it in 2003, and so developed new techniques.  For
compatibility, we used the pcap file format, originally developed for tcpdump, for our raw captured
data.  When we captured our second set of traces in 2007, we needed to
capture at even higher rates, and we used a
specialized capture card. We wrote new capture software using
techniques we had developed in 2003 to allow us to capture above
5Gb/s.

% capture.out a random capture from our nfs fileserver
% tcpdump -r capture.out| wc   
% 1,953,679 22682938 288138719
% ./nettrace2ds --convert --pcap --compress-lzf 0 160877 /tmp/foo.ds   4.74s user 0.45s system 124% cpu 4.159 total
% tshark -r /tmp/capture.out > /dev/null  44.25s user 0.30s system 99% cpu 44.564 total

Tcpdump also includes limited support for conversion of NFS packets.
Wireshark~\cite{wireshark} provides a graphical interface to packet
analysis, and the tshark variant provides conversion to text.  We were
not aware of Wireshark at the time of our first capture, and we simply
adjusted our earlier tools when we did our 2007 tracing.  We may
consider using the Wireshark converter in the future, provided we can
make it run much faster.  Running tshark on a small 2 million packet
capture took about 45 seconds whereas our converter ran in about 5
seconds.  Given conversion takes 2-3 days for a 5 day trace, we can not
afford conversion to slow down by a factor of 9$\times$.

Some of the analysis techniques we use are derived from the database
community, namely the work on cubes~\cite{gray97cube} and approximate
quantiles~\cite{Manku98approximatemedians}.  We considered using a
standard SQL database for our storage and analysis, but abandoned that
quickly because a database that can hold 100 billion rows is
very expensive.  We do use SQL databases for analysis and graphing
once we have reduced the data size down to a few million rows using
our tools.




% LocalWords:  Gbit tcpdump Gb Leung pg lindump driverdump endacedump pcap mmap
% LocalWords:  filesystem DL pps tmpfs gzip NIC IP MiB Endace timestamps lzf du
% LocalWords:  Ghz Opterons TiB PCI Gbps CIFS iSCSI anonymization chunked RPC
% LocalWords:  hashtable Veitch Keeton Ellard readdir RPC's analyses SQL perl
% LocalWords:  DataSeries bzip nfs nullable versioning LAN offline mis HMAC al
% LocalWords:  anonymize parallizable filehandles filehandle anonymized Leung's
% LocalWords:  Ellard's NetApp Wireshark fileserver wc nettrace ds cpu tshark
