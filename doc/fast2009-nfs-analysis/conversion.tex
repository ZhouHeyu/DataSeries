\section{Conversion}
\label{sec:conversion}

Once the data is captured, the second problem is parsing and
converting that data to a easily usable format.  The raw packet format
contains a large amount of unnecessary data, and would require
repeated parsing to be used for NFS analysis.  There are three main
challenges in conversion: representation, performance and anonymization.
Representation is the challenge of deciding the logical structure of
the converted data.  Performance is the challenge of making the
conversion run quickly, hopefully faster than the capture stage.
Anonymization is the challenge of hiding sensitive information present
in the data and is necessary for being able to release traces.

One lesson we learned after conversion is that the converter's version
number should be included in the trace.  As with most programs, there
can be bugs.  Having the version number in the trace makes it easy to
determine which flaws need to be handled.  We recommend the version
control revision ID as a suitable version number.  Related to this is
preservation of data.  An NFS parser will discard data both for space
reasons and for anonymization.  Keeping underlying information, such as
per packet conversion in addition to per NFS-request conversion can
enable cross checking between analysis.  We caught an early bug in our
converter that failed to record packet fragments by comparing the
packet rates and the NFS rates.  

\subsection{Representation}

One option for the representation is the format used in the
Ellard\cite{ellardTraces} traces: one line per request or reply in the
file with field names to identify the different parameters in the RPC.
However, this format works poorly for representing things like
readdir, which have an arbitrary number of response fields.
Therefore, we chose to use a more relational data
structuring~\cite{codd70relational}.  We have a primary data table
with the common fields present in every request or reply, and an
identifier for each RPC.  We then have secondary tables that contain
request-type specific information, such as a single table for RPC's
that include attributes, and a single table for read and write
information.  We then join the common table to the other tables when
we want to perform an analysis that uses information in both.
\fix{1. describe how a single request can generate many rows, perhaps
a little more on the DS format}

The relational structuring improves flexibility, and allows for
analysis that only need a subset of the data to avoid reading
unnecessary data.  For example, an analysis only looking at operation
latency can simply scan the common table.

\subsection{Performance}

To perform the conversion in parallel, we divide the collected files
into groups and process each group separately.  We make two passes
through the data.  First we parse the data and count the number of
requests or replies.  Second we use those counts to determine the
first record-id for each group, and convert the files.  Since NFS
parsing requires the request to parse the reply, we currently do not
parse any request-reply pairs that cross a group boundary.  Similarly,
we do not do full TCP reconstruction, so for NFS over TCP, we only
parse requests or replies that start aligned with the beginning of a
packet. These limitations are similar to earlier work, so we found
them acceptable.  We run the conversion locally on the 8-way tracing
machine rather than a cluster because conversion runs faster than the
1Gbit LAN connection we had at the customer site (the tracing card
does not act as a normal NIC).  Conversion of a full data set (30TiB)
takes about 3 days.

We convert from trace files rather than on-the-fly primarily for
simplicity.  However, a side benefit was that our converter could be
paranoid and conservative, rather than have it try to recover from
conversion problems since we could fix the converter when it was
mis-parsing, or was too conservative.  The next time we trace, we plan
to do more on-the-fly conversion by converting early groups and
deleting those trace files during capture so that we can capture
longer traces.

\subsection{Anonymization}

In order to release the traces, we have to obscure private data such
as filenames.  There are three primary ways to perform the anonymization: 

\begin{enumerate}

\item {\bf map values to unique integers}.  This option results in the
most compact identifiers ($\leq$ 8 bytes), but is difficult to
calculate in parallel and requires a large translation table to
maintain persistant mappings and to convert back to the original data.

\item {\bf map values to hash/hmac}.  This option results in larger
identifiers (16-20 bytes), but enables parallel conversion.  A HMAC
protects against dictionary attacks.  Reversing this mapping requires
preserving a large translation table.

\item {\bf map values to encrypted values}.  This option results in
the longest identifiers since the encrypted value will be at least as
large as the original value.  It is parallizable and easily reversable
provided the small keys are maintained.

\end{enumerate}

We chose the last approach because it preserved the maximum
flexibility, and allowed us to easily have discussions with the
customer about unexpected issues such as writes to what should have
been a read-only filesystem.  Our encryption includes a self-check, so
we can convert back to real filenames by decrypting all hexadecimal
strings and keeping the ones that validate.  We have also used the
reversability to verify for a colleague that they properly identified
the '.' and '..' filenames.

We chose to encrypt entire filenames since the suffixes are specific
to the animation process and are unlikely to be useful to people.
This choice also simplified the discussions about publishing the
traces.  Since we can decrypt, we could in the future change this
decision.

The remaining values were semi-random (IP addresses in the 10.*
network, filehandles selected by the filers), so we pass those values
through unchanged.  All jobs in the customers' cluster were being run
as a common user, so we did not capture uids.  Since they are
transitioning away from that model, future traces would include
unchanged uids and gids.  If there were public values in the traces,
then we would have had to apply more sophisticated
anonymization~\cite{ruoming07anonymization}.



