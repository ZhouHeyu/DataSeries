\section{Storage}
\label{sec:storage}

Having decided to use a relational structuring for our data, we next
needed to decide how to store the data.  There were three primary
options available to us: text, SQL, and our custom binary
format~\cite{DSTechnicalReportSnapshot} for storing trace data.  Text
is a traditional way of storing trace data, however, we were concerned
that a text representation would be too large and too slow.  Having
later converted the Ellard traces to our format, we found that the
analysis distributed with the traces used 25x less CPU time .  This
disparity convinced us that text is an inappropriate format for
storage of our traces.

% cat .../*-log | perl .../DataSeries/doc/fast2008-nfs-analysis/scripts/compression-sum.pl
% last column is size it happened to be compressed to, but used lzf, gzip, bzip2
% nfs-2/set-0: 4260280689888 43738379636 1064156793204 -> 576848706584; 7.46x / 9.23x
% nfs-2/set-1: 3882851266232 42353445184 958237524264 -> 525549755192; 7.47x / 9.21x
% nfs-2/set-2: 4447266037744 51364690388 1127969564332 -> 692186154292; 6.50x / 8.05x
% nfs-2/set-3: 5560956128368 173211639312 1469393844368 -> 1053382989256; 5.44x / 6.67x
% nfs-2/set-4: 3372576162216 76584618188 757255149580 -> 667308820144; 5.17x / 6.19x
% nfs-2/set-5: 3993210407120 16925899320 829238508600 -> 661073679548; 6.07x / 7.29x
% du -b (for entirely gzip compression) ; (a + b / du) / (a + c / du)
% 841832166984    set-0/ ; 5.1x / 6.3x
% 737388264364    set-1/ ; 5.3x / 6.5x
% 852201228772    set-2/ ; 5.2x / 6.5x
% 1106440058916   set-3/ ; 5.2x / 6.4x
% 675664608564    set-4/ ; 5.1x / 6.1x
% 807115490052    set-5/ ; 5.0x / 6.0x

Since we chose a relational structuring for our data, an SQL database
would be a reasonable choice for storing our data.  However, most SQL
databases do not perform compression, and the few that do perform
relatively limited compression. Given that we were expecting to have
very large datasets, we did not feel that SQL would work well.  The
generality of SQL databases also means that we would expect a
substantial penalty when writing complex queries because we would have
to extract all of the data from the database, and they are not
particularly fast at that operation.

Therefore, we were going to need a specialized format that is more
efficient and compact for storing traces.  We had previously developed
a format designed to efficiently store traces.  The format uses a
relational data model, so there are rows of data with each row
comprised of the same typed columns.  A column can be nullable, in
which case there is a hidden boolean field for storing whether the
value is null.  The rows are grouped into extents and compressed as a
unit.  Prior to compression, various transforms are applied to reduce
the size of the data.  First, duplicate strings in the same extent are
collapsed down to a single string.  Second values are delta compressed
relative to either the same value in the previous row.  For example,
the packet time values are delta compressed changing them from large
random numbers to smaller random numbers.  Our format supports a few
other transforms, but they are not relevant to these traces and are
discussed in the TR.

The format is designed for efficient access. Values are packed so that
once an extent is read in, an analysis can iterate over the rows
simply by increasing a single counter, as if for an array of a
structure.  Individual values are accessed by an offset from that
counter and a C++ cast.  Byte swapping if necessary is automatically
performed.  The offset is not fixed, so the same analysis can read
different versions of the data provided the meaning of the fields
hasn't changed.  Efficient access to subsets of the data is supported
by an extent index that is automatically generated so programs can
only read a selected subset of the data.

The format is designed for generality. It supports versioning on the
table types so that an analysis can properly interpret data that may
have changed in meaning.  It has special support for time fields so
that raw values can be stored in seconds, microseconds, nanoseconds,
or whatever the approriate native format, and accessors to that column
can convert to and from the raw format to a few selected formats.

The format is designed for reliability.  It has internal checksums on
both the compressed and the uncompressed data to validate that the
data has been processed appropriately.  Additional details on the
format and comparsions to a wide variety of alternatives can be found
in the technical report~\cite{DSTechnicalReportSnapshot}.
