\section{Capture}
\label{sec:capture}

The first stage in analyzing an NFS workload is capturing the data.
There are three places that the workload could be captured: the
client, the server, or the network.  Capturing the workload on the
clients is very parallel, but is difficult to configure and can
interfere with the actual workload.  Capturing the workload on the
server is straightforward if the server supports capture, 
but impacts the performance of the
server.  Capturing the workload on the network through port mirroring
is about as convenient as capture on the server, and given that most
switches implement mirroring in hardware, has no impact on network or
workload performance.  On fibre networks, even the potential impact of
port mirroring can be eliminated through the use of optical fiber
splitters. Therefore, we have always chosen to capture the
data through the use of port mirroring.

One complication with port mirroring is that usually you are mirroring
both the send and receive directions of full-duplex ports onto a
single mirror port, or that you are mirroring multiple ports bonded
using etherchannel onto a single port. However, advanced switches can
selectively mirror each direction of a port onto particular ports.  We
used this functionality in our 2003 tracing to spread 2-4 one Gbit
links over 2 one Gbit mirror ports.

A second complication that can occur when tracing is buffering on the
switch.  Even if the 1 second byte averages are low enough to fit onto
the mirror ports, if the switch has insufficient buffering, packets
can still be dropped.  We faced this problem in 2004 at a second
site that used switches that only had per-port buffering rather
than shared per-card buffering used by the animation company.  The
sub-second bursts were sufficient to overrun the port buffers leading
to dropped packets.  We therefore needed to use 10Gbit Ethernet packet
capture to reduce the need for switch-side buffering.

A third complication for network capture is overrun of the capture
device.  At low data rates (400-900Mbits, 30-70kpps), it is not
difficult to capture using tcpdump and standard hardware.  However, at
high data rates (5000Mbits, 1,000kpps), traditional approaches are
insufficient. Indeed, Leung\cite{LeungUsenix08} notes 
% pg 215 ``when tcpdump dropped a few packets''
difficulties with packet loss using tcpdump on a 1 Gbit mirror port.
We have developed three separate techniques for packet capture, all of
which work better than tcpdump: {\it lindump}(user-kernel ring
buffer), {\it driverdump}(in-kernel capture to files), and {\it
endacedump}(hardware capture to memory).

\subsection{Lindump}

When we first started packet capture in 2003, we tried to use tcpdump,
as that was the standard approach.  However, we quickly discovered it
was unable to capture packets at the bandwidth that we expected to
see.  While looking to add a faster kernel-user transport, we
discovered that Linux already supported a memory mapped shared ring
buffer for packet capture. We modified the example lindump program to
write out pcap files, and to be able to capture from more than one
interface at the same time.  We used mmap to write the output files to
an in-memory filesystem, and from there we copied and compressed the
files in parallel to disk.  Using an HP DL580G2, a state of the art
4 socket machine circa 2003, lindump was able to capture about 3x the
packets per second as tcpdump and about 1.25x the bandwidth.  Combined
with a somewhat higher burst rate while the kernel and network card
buffered data, this was sufficient for mostly loss free captures at
the animation company, and was the technique we used for all of the
2003 set of traces.

Once our packets are captured into files in tmpfs, we have to save
them to disk, maximizing effective disk space through the use of
compression.  Therefore, we created an adaptive compression scheme.
If the tracing host is mostly idle, we compress with {\tt gzip -9}, as
the backlog of pending files increases, we reduce the compression
algorithm to {\tt gzip -6}, and then to {\tt gzip -1}.  If we continue
to fall behind, we will eventually fall back to just copying the files
out to disk.  In practice this approach increased the effective disk
size by 1.5-2.5x in our experience as the data was somewhat
compressible, but at higher input rates we had to fall back to reduced
compression.

\subsection{Driverdump}

At a second site, our 1Gbit lindump approach was insufficient
because of packet bursts and limited buffering on the switch.
Replacing the dual 1Gbit cards with a 10GbE card merely moved the
bottleneck to the host and the packets would be dropped on the card
before they could be consumed by the kernel.

To fix this problem, we modified the network driver so that instead of
passing packets up the network stack, it would just copy the
packets in pcap format to a file. Then it would
immediately return the packet buffer to the NIC.  A user space
program prepared files for capture, and closed the files on
completion.  We called our solution {\tt driverdump} since it
performed all of the packet dumping in the driver.  Because it was
avoiding the entire kernel IP stack, driverdump could actually capture
packets faster than the kernel could drop them as measured by using
tcpdump with a filter that drops all packets.  We increased the
sustained capture rate over lindump by 2.25x in packets per second to
676,000 packets per second, and 1.5x in bandwidth to 170MiB/s.  We
could handle one second bursts up to 900,000 packets per second, and
215 MiB/s.  This approach was sufficient for nearly lossless capture
to memory at the second site so we decided this was a sufficient
approach.  Since the files were written into tmpfs, we were able to
re-use our technology for compressing and copying the files out to
disk.

\subsection{Endacedump}

In 2007, we captured new traces at the animation company for a new
project.  Since 2003, they had moved to 10GbE, and their NFS servers
were faster. We briefly considered updating our driverdump approach
with modern cards, but decided that we did not want to have problems
with packet capture, so we instead decided to purchase a Endace DAG
8.2X capture card~\cite{endace-cards}.  The Endace card is an FPGA
capture card that copies packets from a 10GbE network directly into
memory.  As a result, it can capture minimal size packets at full
bandwidth, and is intended for doing in-memory analysis of networks.

Therefore, our main challenge became getting the data out from memory
to disk.  The technical contacts at Endace did not expect that we
would be able to get the data written out to disk quickly enough to
keep up.  We integrated our adaptive compression technique into a
specialized capture program, and added the lzf~\cite{lzf} compression
algorithm, as it compresses at about 100MB/s on modern hardware.  We
also upgraded our hardware to an HP DL585g2 with 4 dual core 2.8GhZ
Opterons, and 6 14 disk SCSI trays, 4 with 300GB drives, and 2 with
147GB drives.  Because of the compression, despite having only about
20TiB of disk space, we could usually capture about 30TiB of data.  We
experienced a very small number of packet drops. We did not know that
our capture card limited a single stream to PCI-X bandwidth (8Gbps),
but supported explicit partitioning into two streams.
Newer cards capture 10GbE in a single stream.

\subsection{Discussion}

Our capture techniques are directly applicable to anyone attempting to
capture data from a networked storage service such as NFS, CIFS, or
iSCSI.  Our most advanced techniques are capable of lossless
full-packet capture at 10GbE, although they require the use of special
hardware.  Our simplest technique allows capture at over twice the
rate of tcpdump, and expands the effective size of the disks by 1.5x
through adaptive compression. Our intermediate technique increases the
capture rates by a factor of 2-3x at the cost of development time for
fixing the network driver.  Both the lindump and driverdump code are
available in our source distribution~\cite{DSOpenSource}.  While at
the time we chose high-end servers, today's mid-range servers are the
equivalent of our servers, so there is no reason that everyone should
not be able to do lossless data capture of storage systems.


