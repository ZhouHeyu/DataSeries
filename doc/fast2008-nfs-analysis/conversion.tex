\section{Conversion}

Once the data is captured, the second problem is parsing and
converting that data to a usable format.  Since we have done complete
capture to disk, we have the opportunity to perform a parallel
conversion.  First we have to decide how we are going to represent the
data.  One option is the format used in the Ellard\cite{ellardTraces}
traces: one line per request or reply in the file with field names to
identify the different parameters in the RPC.  However, this format
works poorly for representing things like readdir, which have an
arbitrary number of response fields, and though we didn't initially
implement readdir, we knew that we would want to in the future.
Therefore, we chose to use a more relational data
structuring~\cite{codd70relational}.  We have a primary data table
with the common fields present in every request or reply, and an
identifier for each rpc.  We then have secondary tables that contain
request-type specific information, such as a single table for RPC's
that include attributes, and a single table for read and write
information.  We then join the common table to the other tables when
we want to perform an analysis that uses information in both.

In order to perform the conversion in parallel, we need to divide all
of the collected files into groups and process them in parallel.
Since we need to assign each request or reply a unique id, we actually
make two passes through the data.  First we parse the data and count
the number of requests or replies, we then use those counts to
determine the first number for each group, and hence we can process
the data in parallel.  Since NFS parsing requires the request to parse
the reply, we currently do not parse any request-reply pairs that
cross a group boundary.  Similarly, we do not do full TCP
reconstruction, so for NFS over TCP, we can fail to parse requests or
replies that are not aligned with the start of a packet.  We will
parse multiple request or replies in a single packet.  These
limitations are similar to earlier work, so we found them acceptable.
The 8x speedup we get by converting in parallel means that we can
complete the conversion of a full data set (30TiB) in about 3 days.

We also chose to convert from disk files rather than try to convert
data on the fly.  We did this for simplicity, but it had the side
benefit that we could write our converter to be paranoid and
conservative, rather than have it try to recover from conversion
problems.  This helped greatly when we were handling new trace files
with new failure modes in the underlying data, and has helped reduce
the number of glitches in our converted data.  The next time we
convert data, we plan to extend our conversion tools to work
incrementally, such that we can process the first set of files while
the capture is still running, delete those files, and therefore
capture a theoretically unbounded length of trace.  We briefly
considered parallelizing the conversion across multiple machines, but
when we examined the conversion rate we were getting on our 8 core
machine, we realized that the 1Gbit network connection the tracing
machine had would be the bottleneck and would result in slower
conversion.

... encryption ...



