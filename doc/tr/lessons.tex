\section{Discussion}\label{sec:discussion}

The original motivation for developing \DataSeries{} was our need to
store various types of I/O traces. For over a decade we used a
binary data format for block level traces,
but found this untenable for two
reasons. Firstly, various fields in the traces had been added or
deleted over the years, and worse, some had changed their
meaning. This resulted in significant software engineering overhead
to maintain the multiple internal record structures, and confusion on the
part of those who had to write analyses. Secondly, it was not easily
extensible to some of the new types of data we wished to store. As a
consequence, we designed and built \DataSeries{}, completing the first
release in August of 2003.  

% INCLUDE IN EXTENDED VERSION
%Since that time, we have put a number of
%different data types into \DataSeries{}:
%\begin{enumerate}
%\item {\bf disk block} -- HP-UX block I/O traces, from the HP Labs cello server (see~\cite{Ji03,Uysal03} for fuller descriptions of and example uses of this data).
%\item {\bf process} -- Unix process information converted from running {\tt ps -efl} once a minute.
%\item {\bf NFS} -- NFS RPC information converted from pcap~\cite{pcap} files or equivalents.
%\item {\bf IP} -- IP packet information converted from pcap files.
%\item {\bf LSF} -- Platform Load Sharing Facility (batch cluster) accounting logs.
%\item {\bf sar} -- System activity (cpu/disk/network utilization) records. 
%\item {\bf LSF-metadata} -- Customer specific metadata about their LSF jobs.
%\item {\bf E-mail} -- E-mail messages from the Unix Mailbox format.
%\item {\bf Filesystem} -- Filesystem system call traces, as used in~\cite{Soules05}.
%\end{enumerate}

% Since that time, we have put a number of different data types into \DataSeries{},
% including:
% HP-UX block I/O traces (e.g.,~\cite{Ji03,Uysal03});
% Unix process information (e.g., {\tt ps -efl});
% NFS RPC information;
% IP packet information;
% Platform's Load Sharing Facility (batch cluster) accounting logs;
% % as well as customer specific meta-data about their jobs;
% system activity records (e.g., cpu/disk/network utilization);
% E-mail messages;
% and system call traces (e.g.,~\cite{Soules05}).

Although we have stored many different types of information in \DataSeries{},
very few changes to the initial version have been required.
We have not had to add in any additional data
types beyond the ones described in section~\ref{sec:design},
all of which were in the initial version of \DataSeries{}.
We did add the pack\_scale option in a subsequent version of \DataSeries{},
as we determined it was necessary for improving the compression of
data stored in doubles.  
The pack\_unique option, which was included in the initial version,
has proven to be very useful in improving compression rates.

% COULD INCLUDE IN EXTENDED VERSION
%However, we have had to expand the set of options.  The
%initial implementation of \DataSeries{} represented values with limited
%precision as doubles.  However, they compressed poorly because of the
%effectively random digits at the end of the double.  For this reason,
%we added the pack\_scale option.  Also, the initial implementation
%represented time measured in seconds as a double.  However, there was
%insufficient precision to accurately specify the values as absolute
%times relative to the Unix epoch.  We discovered this problem because
%the pack\_scale option was checking that the scaling would be
%reversible by verifying there was sufficient precision, and the check
%produced an error.
%
%The pack\_unique option has worked surprisingly well.  We initially added it 
%to handle process information, where it provided a modest
%improvement over the compression provided by gzip itself.  When we
%later began tracing NFS data, we found that the protocol had
%changed the RPC id of the various operations, so we chose to put the
%name of the operation in the extent as a string.  Because of the
%pack\_unique option, this choice was essentially free - we have
%measured greater than 10000:1 compression ratios on the variable part of the
%extent containing this string because the pack\_unique option is
%causing \DataSeries{} to collapse all of the occurrences of each string in
%an extent down to a single one.

%We have also written a number of analyses over the various data types.
The vast majority of the analyses we have performed
%have been exactly of the form we expected, 
have been a scan
over a time range of the data in a single extent type.  We have only
wanted joins in a few cases; for example,
to report on the applications with
the most I/O utilization by joining traces of block I/O and Unix process
information.
% COULD INCLUDE IN EXTENDED VERSION
%; and to perform some of the analysis over the NFS data.
%We chose to represent the NFS data by having multiple extent types for
%the different type of operations instead of having a single extent
%type with a large number of nullable fields.  This has worked well as
%the primary table has one entry for each request or response while the
%secondary tables have one entry for each request/response pair.  We do
%this because the secondary data is usually only complete once the
%response has been seen.  
%For example for a read request, we don't know
%how much data was returned until the response is processed. 
%For example, the amount of data returned for a read request
%is not known until the response is processed.
%We assign
%a unique identifier to each request or response to enable the join.
Because all of the extents are sorted in chronological order, we have been able
to use a simplified variant of the sort-merge join; we maintain a
short re-order buffer to allow us to process either based on the
request time or the response time using a standard priority queue, and
then we perform the expected merge between the two tables.  The
pseudo-sortedness of the extents means we can use a single-pass
priority-queue+merge algorithm rather than a two pass
\linebreak[4] sort+merge algorithm.

We have implemented fairly few generic operations because we have not
found them to be useful.  Most of our analyses are more complicated
than could be easily represented in standard SQL, although they have
been straightforward to implement in C++ as a streaming calculation with some
amount of buffering (e.g.,
the sort-merge join described above).  The two generic operations we have
implemented are indexing and statistics over an expression grouped by
a column.  
% INCLUDE IN EXTENDED VERSION
%We implemented indexing because many of our analyses
%operate over over a subset of the data, and reading all of the extents
%only to retain 1\% of the data is very wasteful.  We implement a
%simple indexing algorithm that records for each extent the minimum and maximum
%values of the specified fields.  A separate module reads in all of the
%extents that overlap with the ranges given to it.  Usually the range
%is just a time range, but we have also used it to select out single
%clusters from the LSF data.
%
%The second generic operation we implemented was calculating an
%expression over various columns in an extent and calculating a
%statistic grouped by another column.  We can calculate histogram,
%quantile, exponential moving average and simple statistics over the
%expression's value.  The implementation of this generic operation
%required us to extend our generic field classes to support type
%conversion to double and to support hashing over a field, we already
%had equality from implementing comparisons for generic indexing.

The largest dataset we have is the NFS data.  The primary extent type 
in this data is the common records which store information about each
of the 200 billion request and reply messages. We have secondary tables that
store information about each packet captured, operations that included
file attributes, read and write requests and mount requests.  The total dataset
is about 5TB in size.

As a demonstration of the real-life performance of
\DataSeries{}, consider the following example.  Utilizing a trace of NFS
traffic from a LAN, we performed an analysis of the throughput
effects if servers were instead accessed across a WAN.
% server latency on the throughput of a remote rendering service.  
The analysis read in 45.5 GB of data (406 GB when uncompressed), and
processed 7.6 billion records (each record corresponds to an NFS
transaction).  Using a four year old two-way 2.8 GHz Xeon server, the
entire data set was processed in 11,263 wall clock seconds (about 3
hours), or roughly 675,000 rows per second performing a set of complex
analyses.

As a second demonstration of real life performance we used
\DataSeries{} to build a large set of reports and graphs from LSF
data.  Some of our reports were similar to ones already being created
through queries to a commercial relational database.  Report
generation in \DataSeries{} ran over 50$\times$ faster than the
database report generation.  While we did not investigate precisely
why the database version was slower, it appeared to come from two
sources.  First, \DataSeries{} is entirely targeted at analysis, and
hence runs those calculations very efficiently.  Second, the desired
calculations would require many SQL queries to generate the same
results as the single pass \DataSeries{} calculation, and it is likely
the queries were executed sequentially for simplicity in the program
generating the report.

