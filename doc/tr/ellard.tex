\documentclass[twocolumn, 11pt]{article}
\usepackage{graphicx, fullpage, newcent, epsf, epsfig, graphicx, pictex,
  color, mathptmx, here}
\begin{document}
\section{Experiments}

\subsection{Ellard Traces}

In an effort to experiment with using DataSeries to represent and
analyze traces generated by other people, we converted Daniel Ellard's
Harvard traces\ref{Ellard03} into DataSeries.  The Ellard traces
were originally stored as compressed text files, one record per line.
The first part of each line is a series of fixed fields, followed by a
set of key-value pairs, and finally some debugging information.  As
part of the tools, there is also a scanning program which reads the
trace files and outputs summary information for a trace.

Our evaluation came in two parts.  First, we wrote programs that
converted between the two formats.  The reversable conversion
guaranteed that we were properly preserving all of the information.
We found that the DataSeries files were on average 0.74x the size of
the original files when both were compressed using gzip.  Second, we
wrote an analysis program that implemented the first three examples in
the README that came with the tools.  We found that our analysis
program ran about 76x faster on those data files than the text
analysis program that came with the distribution.  
We also found that if we utilized lzo compression, which decompresses
more quickly than gzip,
our analysis program
ran about 107x faster, in exchange for slightly larger (1.14x) data files.
Other options in the space-time
tradeoff are described as part of the experiments.

\subsubsection{Conversion to and from DataSeries}

The conversion to DataSeries was interesting for two reasons.  First,
it stretched DataSeries in the direction of supporting many nullable
fields, which we had previously resisted going.  Second, it turned out
to be a case study in the difficulties posed by ad-hoc text formats.

DataSeries was designed to follow a relational database model.  As
such it supports null fields, although we have not put any special
optimizations in place to handle them (null fields were previously
just an extra boolean and a test).  While there was a slight space
penalty for having a few null fields, this penalty was mostly removed
by the compression algorithms.  However, when there are many null
fields, it could be much more space efficient to explicitly remove the
null values and create variable length rows before using a generic
compression algorithm.  We call this process null compaction.  We had
previously considered and decided against this feature because it
encourages people to choose schemas that do not follow normal form.
In the Ellard traces, this manifests as not knowning which columns
will be valid given a particular operation type when it is likely that
the operation type and valid fields are fixed.

In our initial conversion of the data, it appeared that the duplicate
elimination performed by the pack\_unique option would compensate for
the additional space used by storing a value for all of the fields.
However, once we had identified all of the fields we needed to store,
the naive implementation resulted in larger DataSeries files than text
files.  There were two options at this point, first, null compaction,
and second, normalizing the data design.  Normalizing the data design
would involve separating out the rows into multiple tables potentially
with keys to have a common and optional tables.  After further
consideration we decided that the most faithful representation would
be a single table, and hence to get small files we implemented null
compaction.

We implemented a reverse conversion program so that people could
continue to use existing scripts, and so we could verify that the
conversion worked properly.  This turned out to be very important, as
the files had a number of glitches in them.  This is a common problem
with under-specified text formats: it is very easy to generate a file
which appears to conform to a specification, but doesn't.  The same
problem can occur with XML, as it is easy to generate invalid XML.
Related to this problem is the lack of a specification; without a
specification, users have no idea what information may be present.
Similarly, in XML without a document type definition (DTD), it is
difficult to understand the meaning of a parsable document.  We solved
the problem of unparsable lines by introducing a ``garbage'' field
into the DataSeries output that stores unparsable lines.  We currently
have code that detects all of the unparsable lines, but if we had
known there would be as many as there were, we would have instead
written the code to throw an exception on parsing errors and store
unparsable lines as garbage.

We experienced a number of these problems in parsing and converting
the Ellard data.  We plan to make checksums (\texttt{sha1} and
\texttt{md5}) available so that people can validate they are working 
with the same input data we used.  We categorize these problems as 
follows:

% Move this so it shows up on same page as reference.
\input{ellard-compression}

\begin{itemize}

\item {\bf Duplicate keys.}  The Ellard traces have key-value pairs on
each line.  We initially asumed that the keys were unique.  However,
we learned that this assumption is incorrect, as a subset of the keys
can occur multiple times on a single line.  Inspection of the code
that ships with the Ellard traces indicates it handles this case by
detecting the duplicate key and silently appending a ``-2'' to the
field name.  We translate these fields as \texttt{\_dup}.

\item {\bf Unknown keys.} There is no list of the keys the Ellard
traces, hence we had to dynamically build up the list of keys that
could be present by attempting to parse a file and generating an error
if we found a new key.  The DataSeries files include (as part of the
extent type) the list of all unique fields observed.

\item {\bf Keys with identical meaning and different names.}  The
Ellard traces parse NFSv3 file handles as a field named \texttt{fh},
but NFSv3 commit file handles into a field named \texttt{file}.
Similarly, file names are parsed as \texttt{fn} for v2 and
\texttt{name} for v3, and offset is parsed as \texttt{offset} for v2
and \texttt{off} for v3. The DataSeries files document these
inconsistencies in a comment for that field.  We could have removed
the inconsistency, but that would have been less faithful to the
original files.

\item {\bf Format changes.}  The Ellard traces document that at some
point they changed the semantics of the \texttt{acc} field from a
character to a bitmask.  To make conversion from DataSeries to the
text format work, we had to determine the date for the change so we
could generate different output depending on the date.  The DataSeries
files always use a bitmask, but if we had encountered this problem, a
version change would clearly indicate the format used in each file.

\item {\bf Format inconsistencies.} The Ellard traces document a
series of fixed fields at the beginning of each line.  However, for
the null operation, the reply format is missing one of those fixed
fields; we had to special case parsing null fields.  Similarly,
different operations print out the fields in different orders.  While
this is valid and correct, it meant we had to special case the
conversion from DataSeries to text to print fields in the appropriate
order.

% MFA: I don't understand what is meant by [0-9]+[0-9]6.
\item {\bf Garbage times.} The Ellard traces specified that times were
in microseconds since the unix epoch, consistent with how NFS
represents these times.  In particular, times were printed as the
regular expression [0-9]+\\.[0-9]{6}, i.e. a series a digits, a period
and then 6 more digits.  Unfortunately, in a number of cases, the
lines did not match that format; 185 of these cases we explicitly
listed, and two numbers showed up sufficiently often that we checked
for them explicitly.  While the number of garbage times is a small
fraction of the total number of lines, it still is worrisome.  We also
subjected the times to a check that they were in a reasonable range of
9 or ten digits for the seconds.  We identified 17 special numbers
that we can't prove are invalid, but some of them are likely to be
network parsing errors; however since we couldn't tell, we parse them
as if valid.  The specific values can be found in the
\texttt{ellardnfs2ds.cpp} source file.

\item {\bf Garbage trailer.}  The Ellard traces end with some
debugging information that has been converted from numbers to XXX's in
the anonymous traces.  Unfortunately, when the line was short, the
cleanup for the trailer was done incorrectly and the debugging
information was left in.  Initially, it looked like the debugging
information was all identical for short packets, but later we found
some cases where it wasn't, so we passed it through as garbage.
Interestingly, the documentation says that the debugging fields can't
be removed because it would break scripts.  This is an advantage of a
format like DataSeries wherein analysis that don't need those fields
would not care if they were removed.

\item {\bf Non-data lines.}  A few lines started with ``XX Funny
line'' We pass these lines through as garbage.

\item {\bf Unknown errors.} There were 36 lines which had some sort of
random error in them.  Most of the errors look like a number of
characters were inserted or deleted at random combining or splitting
multiple lines.  A few of them look like the underlying packet data
was bad, but an output line was still generated as the stable field is
listed as `?'.

\item {\bf Zero blocks.} Eight of the compressed files have long
(multi-MB) blocks of null characters (`$\backslash0$') in them.  We
suspect this came from a toolchain error before we got the files, we
re-downloaded one of the files and verified that it had the block of
nulls in it.  This confused our parser since it saw a line that just
happened to not end with a newline, but thought it had reached the end
of the file as we were using \texttt{fgets}.  We eventually decided
not to try to translate these files, although in theory we could
update our program so that it would properly parse them, and pass them
through as garbage.

\end{itemize}

\subsubsection{Compression comparison}

Table~\ref{table:ellard:compression} shows the DataSeries compression results
relative to the original Ellard traces.  The compression difference
using bzip2 compression is slightly lower than with gzip, the
DataSeries files are 0.76x smaller than the text files compressed
using bzip2.  We compared the lzo files to the gzip compressed files
since for the text files compression with lzo would offer no benefit,
the files would be larger and the wall time for analysis would be the
same.  For gzip we tested with extent sizes of 64k, 128k and 512k.
For lzo, we tested with extent sizes of 64k and 128k.  We didn't
bother to calculate compression ratios for lzo-512k because the
performance is no better than gzip, and the compression would be
worse.  Interestingly, the ratios are not constant across the
different trace sets:

We have not investigated what causes the difference in the compressed
file sizes.  We have observed that for the small extent sizes
(64k/128k) the compressed extents are very small (5-10k), which means
that some of the DataSeries per-extent overhead may be contributing to
the larger size, as well as the compression algorithms may not have
enough data to even fill their window.

\subsubsection{Performance comparison}

For the performance comparison, we implemented a subset of the
analysis performed Ellard's \texttt{nfsscan} program. In particular one
that can perform the first three of the five example questions
presented in the EXAMPLES file that comes with Ellard's trace tools.
This analysis turned out to be very simple, it is just counting the
number of requests performed of each client of each type.  We chose to
implement this over the integer operation id, rather than the string,
and so wrote a short table that converted NFSv2 and NFSv3 operation
id's into a common space. The performance comparison was done using
DataSeries revision 61f07e212acb972da6c603bed82ab2ec5ca1b731 from
2008-01-21.

Our initial implementation did not perform as expected.  In particular,
we expected to see
the CPU utilization exceed 100\% during execution because the
analysis and the decompression steps were overlapped.  Further
investigation using \texttt{oprofile} indicated that the analysis module was
only using 4\% of the total CPU time; 96\% of the application's time
was going into decompressing the extents.  We therefore decided it was
time to implement parallel decompression so that we could take
suitable advantage of our multi-core machines.

The implementation on multi-core machines appeared as if it would be
straightforward, we implemented a standard pipeline, with one thread for
prefetching extents off disk, and $n$ threads for decompressing
extents, defaulting $n$ to the number of CPU cores.  Each stage in the
pipeline had a maximum memory capacity.

Experiments with this scheme indicated that the performance had high
variance.  After studying the problem we identified two issues.
First, the analysis thread could be pre-empted by a decompression
thread, and second, the thread decompressing the first extent in the
decompression queue could be pre-empted by any of the other threads.
Either of these situations could result in stalls in processing, and
instability in the performance.  Eventually we decided to detect
these two conditions, the second by noting that the first extent in
the decompression queue is not ready, and the current decompression
thread is working on an extent far down in the queue.  In either of
these two cases, the decompression thread will call \texttt{sched\_yield} to
try to get the more important thread running again.

This use of \texttt{sched\_yield} is an inferior solution because it
can result in many system calls that end up doing nothing, or simply
transfer us from running one thread that doesn't matter to a different
thread that doesn't matter.  With the existing threading interface,
the only other option to try would be to use priorities, however it is
not clear that priorities are sufficiently pre-emptable across CPUs to
have a useful effect.  If we were to extend the kernel threading
interface, there are two obvious possibilities for an improved
interface.  The first is what we call a directed yield, it would be a
variant of \texttt{sched\_yield}, but would specify a thread that should start
running, the call would transfer control to that thread if it is not
running, or do nothing otherwise.  The second possibility is process
local priorities, this would allow us to increase and decrease the
priorities dynamically during a run (which is currently not allowed as
threads usually can't increase their priority), and it would isolate
this process from other processes so that decreasing a thread's
priority would not cause other processes to run in preference to the
thread with lowered priority.  It is unclear which of these solutions
would work best, or how they could be made properly composable so that
in more complicated pipeline graphs the ``right thing'' can still
happen.

All of our experiments were performed on a DL365g1 with 8 GB memory,
2x 2.4GHz dual-core Opteron 2216 HE.  Data was stored on nfs.
Ellard's \texttt{nfsscan} program was run \texttt{zcat (or bzcat) $|$
nfsscan -t0 -BC -}, so we get separate times for the decompression and
nfsscan execution, but a single elapsed time.  The detailed
measurements can be found in the DataSeries distribution in
\texttt{doc/tr/ellard-details.tex}.

\input{ellard-summary}

We compare below the performance of running \texttt{nfsscan} either
with gzip or bz2 inputs, and the performance of running DataSeries
with bz2, gzip, and lzo inputs.  Interestingly, for the gzip inputs,
the scheduler chose to keep the gunzip and the \texttt{nfsscan}
processes on the same CPU.  For bzip2, it used different CPUs, which
meant that \texttt{nfsscan} ran somewhat slower, presumably because
the data had to be copied between CPUs, bzip2 has a larger block size,
and the buffering was insufficient to smooth out the difference.

Table~\ref{tab:summary} presents the summary results, showing the
impressive speedup and reduction in CPU time that can be achieved by
using DataSeries.  The different sizes specified after the compression
algorithm for the DataSeries rows are the extent sizes. 
The substantial increase in system time for dealing
with large extents for bzip2 is a result of glibc's use of mmap/munmap
for large allocations.  Every extent results in a separate pair of
mmap/munmap calls to the kernel and hence a substantial about of page
zeroing in the kernel.  The detailed measurements can be found 
in the DataSeries distribution in \texttt{doc/tr/ellard-details.tex}.

\end{document}
