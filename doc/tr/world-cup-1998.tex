\subsection{1998 World cup traces}\label{sec:world-cup-1998}

% Timing measurements are stored in wcanalysis.cpp

The 1998 World cup traces\cite{ita-wcweb98} are one of the largest
publically available web traces.  They were collected from the many
web servers for the 1998 World Cup games.  We converted the traces
from the special raw format used for them into DataSeries, and also
created a version of the {\tt checklog} program that comes with the
traces as a comparison point for analysis.  For these traces, we found
that conversion to DataSeries files were on average 0.93$\times$ the
size of the original files. The analysis program used about
1.17$\times$ less CPU time, and ran about 3.46$\times$ faster on the
complete data set.

The analysis program used about
1.04$\times$ less CPU time, and ran about 2.60$\times$ faster on the
complete dataset in the original version.  After optimization, the
analysis program used about 1.17$\times$ less CPU time, and ran about
3.46$\times$ faster.  Compared on a subset of data that fit entirely
in memory,

\subsubsection{Conversion to and from DataSeries}

The conversion to and from DataSeries was quite straightforward.  The
traces use a simple fixed record format of 4 32 bit integers and 4 8
bit integers.  The integers are unsigned in the original format, but
since DataSeries only supports signed 32 bit integers, we converted
to a signed format, and convert backwards during analysis.

As with most of our converters, we convert both backwards and forwards
to verify that the conversion was correct.  The conversion results
turned out to be very strange; while overall DataSeries compressed 2\%
better, some of the files ended up being smaller while others ended up
being larger.  Further examination identified five factors that were
affecting the file size:

\begin{itemize}

\item {\bf Compression level}.  The original gzipped World Cup files were
compressed using {\tt gzip -6}, as that is the default compression
level.  DataSeries files default to {\tt gzip -9}.  We simply
re-compressed all of the original files as level 9.  However, this
points out the importance of defaults as people have been downloading
and using needlessly oversized files since they were released.

\item {\bf Endianness}.  The original World Cup files were stored
in big-endian format.  DataSeries files are stored in the native
endianness of the host, which on x86 processors is little-endian.

\item {\bf Field order}.  The original World Cup files were stored
with the four 32-bit fields first and then the 4 8-bit fields.
DataSeries happened to store the files with all the 8-bit fields first
followed by the 32-bit ones.

\item {\bf Field padding}.  The original World Cup files included
no padding.  DataSeries padded the files to an 8 byte boundary, so
added in an extra 4 bytes of zeros to each record.

\item {\bf Relative field packing}.  The original World Cup files stored
the time field as absolute seconds.  DataSeries includes an option to
store fields calculated relative to the previous value.

\end{itemize}

Table~\ref{table:wc1998:compression} shows the overall compression of
the different options.  The \{le,be\}-\{bts,stb\}-gz files are the
original data files stored in little or big endian format (le/be) and
big to small or small to big (bts/stb) ordering.  The original traces
are be-bts-gz.  The DataSeries file without any options is ds-base,
and then options can be added on for reducing the field padding to the
maximum column size (mcs), packing the fields in big to small order
(bts), or packing the time field self-relative (tsr).

% plots and tables can be re-generated using graphs/wc1998/plot.hg
% files can be re-generated by running wcweb2ds with an extent size of 1000000
% and by changing pack_ to xack_ in the type definition.

\begin{table}
\centering
\begin{tabular}{|l|r|r|r|}\hline

               &            & ratio to & ratio to  \\
    dataset    & size (MiB) & ds-base  & be-bts-gz \\
\hline
ds-mcs-bts-tsr & 7648.98    & 1.133    & 1.077     \\
ds-mcs-tsr     & 7722.78    & 1.122    & 1.067     \\
ds-bts-tsr     & 7890.12    & 1.098    & 1.044     \\
ds-tsr         & 8065.99    & 1.074    & 1.021     \\
le-bts-gz      & 8152.47    & 1.063    & 1.011     \\
ds-mcs-bts     & 8183.98    & 1.059    & 1.007     \\
be-bts-gz      & 8239.15    & 1.052    & 1.000     \\
le-stb-gz      & 8391.09    & 1.032    & 0.982     \\
be-stb-gz      & 8411.46    & 1.030    & 0.980     \\
ds-mcs         & 8423.56    & 1.029    & 0.978     \\
ds-bts         & 8439.72    & 1.027    & 0.976     \\
ds-base        & 8663.78    & 1.000    & 0.951     \\
\hline
\end{tabular}


\caption{Compression options for the data files sorted from the smallest
to the largest.  Gzipped files can be stored little or big endian
(le/be) and small to big or big to small (stb/bts).  DataSeries files
can be stored with the fields padded only to the maximum column size
(mcs), with the fields stored big to small (bts), or with the time
field packed self relative (tsr). be-bts-gz is the original 1998 World
Cup trace format.}

\label{table:wc1998:compression}
\end{table}

\subsubsection{Original file compression experiments}

\begin{figure}
\epsfig{width=3.2in, angle=0, file=graphs/wc1998/reordering-options.ps}
\caption{Reordering options for the original data files.
The data can be stored in little or big endian format (le/be); it can
also be stored with the small fields before the big ones or the
reverse (stb/bts)}
\label{fig:wc1998:original-data-reordering}
\end{figure}

Figure~\ref{fig:wc1998:original-data-reordering} shows the four
possible options for packing the data before compression in the
original data format.  Since the dataset includes some empty fields,
we always get some files that compress the same regardless of the
packing (to 20 bytes, the minimal gzip file size).

The top two lines compare the small to big and big to small ordering
options holding the endianness constant.  Since both lines are
entirely positive, this shows us that we always prefer the
big-to-small ordering for this dataset (if the ratio is $>$ 1 then the
denominator is smaller and hence the preferrable choice), regardless
of endianness.  However, we note that changing the ordering makes more
of a difference if the files are little endian.

The bottom two lines compare the endianness choices holding the field
ordering constant.  Since the lines are mostly below zero, we in
gneneral prefer the little endian files to the big endian files, and
the preference is more pronounced for the big-to-small ordering.

\subsubsection{DataSeries file compression experiments}

The first DataSeries graph looks at the overhead imposed by the
DataSeries file format.  For this experiment, we pruned out the 5 tiny
files (4 empty, and 1 $<$4k compressed) in the dataset as DataSeries
imposes a large overhead on very tiny files since it includes the type
information in each file.  In particular, for an empty file, the
overhead is about 80x as at DataSeries files are 1.6k and the original
files are 20 bytes.  Figure~\ref{fig:wc1998:ds-overhead} shows
the overhead imposed by DataSeries with both the big-to-small and the
small-to-big orderings for the files, and using $10^6$ byte extent
sizes.  We can see that DataSeries imposes a 0.2\%-0.5\% overhead on
the files if the underlying data is identical.  We manually verified
that a few files were in identical except for the type extent at the
beginning of the file, the extent headers that occurred at the middle
of the files, and the trailer at the end.

\begin{figure}
\epsfig{width=3.2in, angle=0, file=graphs/wc1998/ds-overhead.ps}
\caption{Overhead imposed by DataSeries. The record format is constrained
to be identical to the original data.  DataSeries imposes a
0.2\%-0.5\% overhead.}
\label{fig:wc1998:ds-overhead}
\end{figure}

The second graph shown in figure~\ref{fig:wc1998:ds-opts-1} looks
at the effect of the field packing (mcs) and field ordering (bts)
options for DataSeries.  We compare the file sizes measured by turning
on each of the options individually, and then both at the same time.
The results are much as would be expected; eliminating the extra
padding (mcs) gives us a flat 2-3\% compression improvement.
Switching to big-to-small (bts) field ordering gives a 1-4\%.  The
options combine to give us a 5-7\% compression improvement over the
base DataSeries case.  The one oddness is the couple of points that
are below 1.  These come from the empty files.  We changed the options
to the packing by changing the XML option from pack\_* to xack\_* so
as to minimize the change in the size of the type extent.  However,
having all three options specified as xack\_* compresses slightly
better (4 bytes) than having one or two options as pack\_* and the
others as xack\_*, resulting in the negligably below 1 ratio.

\begin{figure}
\epsfig{width=3.2in, angle=0, file=graphs/wc1998/ds-opts-1.ps}
\caption{Turning on the field compaction (mcs) and ordering (bts) options
in DataSeries.  The compression options are mostly independent, so the
resulting compression of turning both on is multiplicative.}
\label{fig:wc1998:ds-opts-1}
\end{figure}

The third graph shown in figure~\ref{fig:wc1998:ds-opts-2} looks
at the effect of the time self-relative packing option and then
combines it with the field packing (mcs) and field ordering (bts)
options.  Time self relative gives a much more substantial improvement
in some cases -- by itself it can give a 20\% compression improvement.
However it does not operate nearly as independently as the first two
options, in particular turning on both bts and mcs packing is not much
of an improvement over just mcs packing.

\begin{figure}
\epsfig{width=3.2in, angle=0, file=graphs/wc1998/ds-opts-2.ps}
\caption{Turning on the time self-relative (tsr) option and then the
field compaction (mcs) and ordering (bts) options in DataSeries.
With time self-relative packing, the options are no longer independent,
and overall time self-relative gives a much more substantial improvement
than the other options.}
\label{fig:wc1998:ds-opts-2}
\end{figure}

The fourth graph shown in figure~\ref{fig:wc1998:ds-opts-3} looks at
just the effect of turning on the big-to-small packing option with the
field compaction (mcs) and time self-relative (tsr) options already
turned on.  This graph shows why the benefit of big-to-small is so
minor once the other options are on.  The big-to-small option
interferes in some cases with the time self-relative option.  This
figure leads to the idea that different extents could be packed using
different field orderings as in DataSeries, each extent boundary could
be stored in different ways.

\begin{figure}
\epsfig{width=3.2in, angle=0, file=graphs/wc1998/ds-opts-3.ps}
\caption{Comparing just the field ordering option (bts) with both the
time self-relative (tsr) and field compaction (mcs) options enabled.
Packing big to small is a win, but not by much.}
\label{fig:wc1998:ds-opts-3}
\end{figure}

The fifth graph shown in figure~\ref{fig:wc1998:ds-size-cmp} looks at
whether or not there is any size correlation to the improvment
provided by the compression options.  Visually the graph shows no
significant correlation between size and compression ratio as the
values are just scattered within a range.

\begin{figure}
\epsfig{width=3.2in, angle=0, file=graphs/wc1998/ds-size-cmp.ps}
\caption{Determining if any of the compression options correlate with
size of the file.  Visually they do not appear to.}
\label{fig:wc1998:ds-size-cmp}
\end{figure}

\subsubsection{Compression conclusions}

The work in this section showed that the field compaction option had a
small but consistent effect on the compressibility of the data.  We
would recommend that any extent type that does not have any 8 byte
fields always turn on this option.  In fact, the option could be
turned on in general, but it will have no effect (other than consuming
a small amount of space in the extent type) if there are 8 byte fields
in the record type, or if the record size is otherwise a multiple of
8.

The work in this section also shows that field ordering can have a
substantial effect on the compressibility of an extent.  On every
extent boundary, the fields that access the extent get a chance to
re-calculate their offsets into the data.  This means that for graphs
like figure~\ref{fig:wc1998:ds-opts-3} we could in theory make it so
that all files are compressed as well as just having the mcs+tsr
options enabled.  Indeed, just choosing a per-file field ordering that
would match the best ordering for the file would improve the
compression.  However, we could potentially do even better as each
extent in a single file could choose the best ordering.  Moreover, we
have only tested a subset of the possible ordering options.  For this
dataset, there are actually $5!*4!$ possible orderings that don't
introduce additional padding.  This comes from there are 5 32-bit
units (4 actual and 4 8-bit) that could have any order, and then the 4
8-bit values could be ordered in any order.  We hypothesize that
correlations between fields will correlate with the amount of
compression that could be achieved, and hence it may be possible to
work out much better field orderings without evaluating an exponential
number of values.

\subsubsection{Analysis performance}

% detailed measurements in src/analysis/wcanalysis.cpp

We re-implemented the {\tt checklog} program that comes with the 1998
World Cup traces.  That program calculates a couple of the values in
multiple ways.  We duplicated this calculation because we wanted the
programs to be comparable, even though performing the calculation is
unnecesary. When running the program we found that the DataSeries
implementation ran 2.60$\times$ faster.  This result was expected as
the analysis performed was fairly simple, and as DataSeries used
multiple CPUs for decompression, it was able to run faster.  However,
we also found that the implementation used about 1.04$\times$ less
CPU.  This was unexpected given that DataSeries was using a virtual
function call for every row, was applying additional if statements to
test for null fields on every access, and was having to process the
extents separately.  We examined the CPU utilization using oprofile
and valgrind and found that the original code had four problems:

\begin{itemize}

\item {\bf Default to non-optimized compilation}. The {\tt checklog}
program defaulted to non-optimized compilation.  This led to a much
larger initial difference measured between the programs.  The numbers
reported above are after this was fixed in the Makefile.  DataSeries
defaults to optimized builds for exactly this reason.

\item {\bf Non-integrated decompression}.  The {\tt checklog} program
used a separate process to perform the decompression, this led to
slightly increased system time to pass the data over the pipe.

\item {\bf Very slow byte-swapping routine}.  The {\tt checklog} program
had to byte-swap every record, and the implementation used for
byte-swapping was very slow.  Moreover, the implementation copied the
data rather than doing the byte-swapping in-place, further increasing
the overhead.

\item {\bf Slow per-record fread}.  The {\tt checklog} program called
fread for each record.  While we didn't expect this to be slow, it
turned out that the implementation is slow, and so calling fread for
each record is a poor choice.

\end{itemize}

It turned out to be an artifact that caused the DataSeries analysis
program to use slightly less cpu time than the {\tt checklog} program.
Examining the number of instructions showed that {\tt checklog} used
only 959 million instructions to perform the analysis, while
DataSeries was taking 1,775 million instructions to perform the same
analysis.  Optimizing the {\tt checklog} program so that it performs
better would result in it using less CPU time than DataSeries.
Parallelizing decompression would be more difficult since we could
only parallelize on a per-file basis and each individual file is
fairly large.

However, we did choose to examine how we could optimize the DataSeries
program.  The analysis indicated that there were two sources of
slowdown: if-tests for possibly null fields, and virtual function
calls.  We modified the {\tt wcanalysis} program to have a special
version of the fields that skipped the null test, and expanded out the
getExtent function so that there were no per-row virtual function
calls.  This led to the measurements shown in
figure~\ref{table:wc1998:optimize} that show a 1.6$\times$ reduction
in instructions and a potential 1.1$\times$ improvement in runtime.
Most of the runtime improvement came from skipping the nullable tests,
so we implemented a templatized version of the extent fields so that
the nullable check could be turned of if the analysis can not deal
with null fields.  The templatized version performed identically to
the special case versions of the fields written for the
direct-\{int32,byte\} fields use by the world cup analysis.

\begin{table*}
\centering
\begin{tabular}{|l|r|r|r|r|r|}
\hline
             & user     &       & \multicolumn{2}{c|}{billion instructions} & \\
measurement  & time (s) & ratio & total & for analysis & analysis ratio \\
\hline
gunzip       & 1.92     &   -   &   n/a &   n/a &   -   \\
checklog     & 1.02     &   -   & 3.060 & 0.960 &   -   \\
\hline
original     & 1.92     & 1.000	& 5.134 & 1.775 & 1.000 \\
direct-int32 & 1.83     & 1.049 & 4.784 & 1.424 & 1.247 \\
+direct-byte & 1.77     & 1.085 & 4.588 & 1.228 & 1.445 \\
+non-virtual & 1.72     & 1.116 & 4.440 & 1.081 & 1.642 \\
\hline
gcc-4.3      &  -       &   -   &   -   & 0.875 & 2.028 \\
\hline
\end{tabular}

\caption{
Measurements for the world cup analysis.  gunzip and checklog were use
in a pipeline.  Except for the gcc-4.3 measurement which was taken on
a separate debian (lenny) machine, the timings and instructions were
measured on a HP nw9440 T2600 cpu running debian etch.  Instruction
counts were within 100,000 instructions between the two debian
machines if we used gcc-4.1 on both. Ratios are relative to the
original measurement in both cases; the direct-byte and no-virtual
rows are cumulative optimizations with the previous rows.}

\label{table:wc1998:optimize}
\end{table*}

For consistency in our measurements, we measured the performance of
the tools for the full data-set under RHEL4 using the same system our
other measurements ahve been taken on, a 2.4GhZ dual-cpu, dual-core
DL365 with opteron 2216 HE processors.  Given our experience above, we
measured the instruction counts using valgrind, and found that {\tt
checklog} was using 800m instructions for the analysis whereas {\tt
wcanalysis} was using 1,357m instructions.  Compiling with gcc-4.3,
and using the non-virtual row processing would drop the instruction
count to 801m, indicating that gcc-3.4 on RHEL4 has a higher
abstraction penalty than gcc-4.1 and 4.3.  Since this would not model
the performance that would be measured on stock RHEL4, we chose to
take the measurements for the full tests using gcc-3.4 compilation,
templatized extent fields, and the virtual function call per row since
this is the recommended implementation pattern (or at least will be
once the templatized implementation is well tested).

We ran all of the programs over the full data set, and then ran them
over a subset of the data that would fit in memory as we discovered
that when compressed with lzo, we were being limited by the disk read
rate for performing our analysis.  The results are shown in
figure~\ref{table:wc1998:analysis}.  They show that we are limited in
the analysis performance by the speed of the analysis, hence the
switch to gcc-4.3 should bring a substantial improvement.  We found
that the analysis on the full dataset might have been disk limited,
but established that it only was for the lzo compression algorithm,
the gz algorithm compressed just barely well enough to not be disk
limited.  We expect that with gcc-4.3 we would be disk limited for
this analysis.

\begin{table*}
\centering
\begin{tabular}{|l|r|r|r|r|r|r|r|}
\hline
measurement         & user & system  & cpu    & ratio & wall   & ratio & cpu-util \\
\hline
gunzip,checklog    & 364.88 & 45.30 & 410.18 & 1.00 & 428.93 & 1.00 &  96 \\
original-full-gz    & 352.35 & 41.46 & 393.81 & 1.04 & 165.03 & 2.60 & 239 \\
template-full-gz    & 308.76 & 42.10 & 350.86 & 1.17 & 123.81 & 3.46 & 283 \\
\hline
checklog-subset     & 184.56 & 19.75 & 204.32 & 1.00 & 208.22 & 1.00 &  98 \\
template-subset-gz  & 152.30 & 20.67 & 172.97 & 1.18 &  60.63 & 3.43 & 285 \\
template-subset-lzo & 117.66 & 21.31 & 138.97 & 1.47 &  60.36 & 3.45 & 230 \\
\hline
\end{tabular}

\caption{
Performance measurements over the full world cup dataset, or a 
subset of the data (files matching wc\_day$[12345]*$).  The dramatic
wall clock time improvement between the original and template version
shows that we are being limited by analysis performance.  This
observation is repeated by the complete lack of wall clock time
improvement shown by switching to the lzo decompression algorithm.
Measurements using the lzo compression algorithm on the full dataset
were slower because we were limited by the disk read performance, and
so were not further measured.  All measurements were taken with
1million byte extent sizes to match the sizes use in the compression
comparison.
}

\label{table:wc1998:analysis}
\end{table*}
